{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e211aa1f-347a-4b1e-94b7-ab17fe1fade2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install ultralytics supervision huggingface_hub opencv-python torch pandas matplotlib seaborn\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Thermal Human Detection using YOLOv8 - Jupyter Notebook Version\n",
    "================================================================\n",
    "A comprehensive system for detecting humans in thermal imagery using YOLOv8.\n",
    "\n",
    "Author: [Your Name]\n",
    "Date: December 2025\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# Install Required Packages\n",
    "# ============================================================================\n",
    "# Run this cell first to install all dependencies\n",
    "\n",
    "\"\"\"\n",
    "!pip install ultralytics supervision huggingface_hub opencv-python torch pandas matplotlib seaborn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11158b4a-2723-467b-83c5-ff993259a871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully!\n",
      "‚úì PyTorch version: 2.9.1+cu130\n",
      "‚úì CUDA available: True\n",
      "‚úì Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Import Libraries\n",
    "# ============================================================================\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from ultralytics import YOLO\n",
    "from supervision import Detections\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"‚úì Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6bb18a8-de18-406f-a59c-4b0fae573193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration loaded successfully!\n",
      "‚úì Output directory: outputs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for the project\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    MODEL_REPO = \"pitangent-ds/YOLOv8-human-detection-thermal\"\n",
    "    MODEL_FILENAME = \"model.pt\"\n",
    "    BASE_MODEL = \"yolov8n.pt\"\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 16  # Reduced for notebooks\n",
    "    EPOCHS = 30\n",
    "    IMG_SIZE = [640, 480]\n",
    "    OPTIMIZER = \"AdamW\"\n",
    "    LEARNING_RATE = 3e-5\n",
    "    WARMUP_EPOCHS = 10\n",
    "    CONFIDENCE_THRESHOLD = 0.6\n",
    "    \n",
    "    # Data augmentation\n",
    "    AUGMENTATION = {\n",
    "        \"hsv_h\": 0.015,\n",
    "        \"hsv_s\": 0.7,\n",
    "        \"hsv_v\": 0.4,\n",
    "        \"degrees\": 10.0,\n",
    "        \"translate\": 0.1,\n",
    "        \"scale\": 0.5,\n",
    "        \"flipud\": 0.5,\n",
    "        \"fliplr\": 0.5,\n",
    "        \"mosaic\": 1.0,\n",
    "        \"mixup\": 0.1,\n",
    "    }\n",
    "    \n",
    "    # Paths\n",
    "    DATA_YAML = \"data.yaml\"\n",
    "    OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "# Create output directories\n",
    "Path(Config.OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "Path(f\"{Config.OUTPUT_DIR}/images\").mkdir(exist_ok=True)\n",
    "Path(f\"{Config.OUTPUT_DIR}/videos\").mkdir(exist_ok=True)\n",
    "Path(f\"{Config.OUTPUT_DIR}/plots\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuration loaded successfully!\")\n",
    "print(f\"‚úì Output directory: {Config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ddccf8-bdf7-4a09-bd77-b63a37f84401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to download pre-trained model (uncomment code above)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Download Pre-trained Model (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "def download_pretrained_model():\n",
    "    \"\"\"Download pre-trained thermal detection model from HuggingFace\"\"\"\n",
    "    print(\"Downloading pre-trained model from HuggingFace...\")\n",
    "    try:\n",
    "        model_path = hf_hub_download(\n",
    "            repo_id=Config.MODEL_REPO,\n",
    "            filename=Config.MODEL_FILENAME\n",
    "        )\n",
    "        print(f\"‚úì Model downloaded to: {model_path}\")\n",
    "        return model_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error downloading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Uncomment to download pre-trained model\n",
    "# pretrained_path = download_pretrained_model()\n",
    "# pretrained_model = YOLO(pretrained_path)\n",
    "\n",
    "print(\"Ready to download pre-trained model (uncomment code above)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c35cf1-21d9-441b-b7a6-40b4499a0a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8 base model...\n",
      "‚úì Loaded model: yolov8n.pt\n",
      "‚úì Model summary:\n",
      "YOLOv8n summary: 129 layers, 3,157,200 parameters, 0 gradients, 8.9 GFLOPs\n",
      "(129, 3157200, 0, 8.8575488)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load Base Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading YOLOv8 base model...\")\n",
    "model = YOLO(Config.BASE_MODEL)\n",
    "print(f\"‚úì Loaded model: {Config.BASE_MODEL}\")\n",
    "print(f\"‚úì Model summary:\")\n",
    "print(model.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee03d759-2f6b-425f-822b-abad0ebcf562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "Training Configuration:\n",
      "--------------------------------------------------\n",
      "  batch: 16\n",
      "  epochs: 30\n",
      "  imgsz: [640, 480]\n",
      "  optimizer: AdamW\n",
      "  cos_lr: True\n",
      "  lr0: 3e-05\n",
      "  warmup_epochs: 10\n",
      "  hsv_h: 0.015\n",
      "  hsv_s: 0.7\n",
      "  hsv_v: 0.4\n",
      "  degrees: 10.0\n",
      "  translate: 0.1\n",
      "  scale: 0.5\n",
      "  flipud: 0.5\n",
      "  fliplr: 0.5\n",
      "  mosaic: 1.0\n",
      "  mixup: 0.1\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úì Training on device: cuda\n",
      "Ultralytics 8.3.241 üöÄ Python-3.11.14 torch-2.9.1+cu130 CUDA:0 (NVIDIA GeForce RTX 3060, 11909MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=data.yaml, degrees=10.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=[640, 480], int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=3e-05, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/igris/Files/Image processing projects/thermal_obj_detect/runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=10, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "WARNING ‚ö†Ô∏è updating to 'imgsz=640'. 'train' and 'val' imgsz must be an integer, while 'predict' and 'export' imgsz may be a [h, w] list or an integer, i.e. 'yolo export imgsz=640,480' or 'yolo export imgsz=640'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3564.3¬±935.5 MB/s, size: 60.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/igris/Files/Image processing projects/thermal_obj_detect/train/labels.cache... 7498 images, 546 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7498/7498 18.8Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/igris/Files/Image processing projects/thermal_obj_detect/train/images/FLIR_05639_jpeg.rf.0708d187f69bd50c3a095651a6f903d3.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/igris/Files/Image processing projects/thermal_obj_detect/train/images/FLIR_05639_jpeg.rf.18fd53b784425ca0c01268cec5ce3a57.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/igris/Files/Image processing projects/thermal_obj_detect/train/images/FLIR_05812_jpeg.rf.4bbb9b9e044a1b8654a8c14ce6678b34.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/igris/Files/Image processing projects/thermal_obj_detect/train/images/FLIR_05812_jpeg.rf.87558d3f22123a4d1f73b19b3ad0cede.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/igris/Files/Image processing projects/thermal_obj_detect/train/images/FLIR_09055_jpeg.rf.a48181dd884582543ede034964449530.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/igris/Files/Image processing projects/thermal_obj_detect/train/images/FLIR_09055_jpeg.rf.fad99abaf35ed1161762e64d3d6c89b5.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1290.9¬±350.7 MB/s, size: 63.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/igris/Files/Image processing projects/thermal_obj_detect/test/labels.cache... 553 images, 42 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 553/553 393.6Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/igris/Files/Image processing projects/thermal_obj_detect/test/images/FLIR_07525_jpeg.rf.249e52651afbf0726fe5eefee21443ec.jpg: 1 duplicate labels removed\n",
      "Plotting labels to /home/igris/Files/Image processing projects/thermal_obj_detect/runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=3e-05, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/igris/Files/Image processing projects/thermal_obj_detect/runs/detect/train2\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/30      2.52G      1.878      2.234      1.332        131        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 8.1it/s 58.2s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 9.5it/s 1.9s0.1s\n",
      "                   all        553       4232      0.402      0.256      0.275      0.128\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/30      2.77G      1.787      1.818      1.272         88        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 8.7it/s 53.8s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 9.3it/s 1.9s0.1s\n",
      "                   all        553       4232      0.568      0.425      0.455      0.227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/30      2.78G      1.717      1.676      1.235        109        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 8.5it/s 54.9s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 9.3it/s 1.9s0.1s\n",
      "                   all        553       4232      0.586      0.482      0.505      0.251\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/30      2.78G      1.671      1.578      1.212        108        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.0it/s 51.8s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.0it/s 1.8s.2s\n",
      "                   all        553       4232      0.657      0.509      0.568       0.31\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/30      2.78G      1.618       1.49      1.187         78        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.0it/s 51.9s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 9.6it/s 1.9s0.1s\n",
      "                   all        553       4232      0.668      0.557      0.613      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/30      2.78G       1.59      1.417      1.171         91        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.2it/s 51.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.1it/s 1.8s.1s\n",
      "                   all        553       4232      0.693      0.546      0.614      0.332\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/30      2.78G       1.56      1.365      1.156        137        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.2it/s 51.1s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.3it/s 1.7s.2s\n",
      "                   all        553       4232      0.725      0.557      0.644      0.364\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/30      2.78G      1.534      1.316      1.144        145        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 8.7it/s 54.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.2it/s 1.8s.2s\n",
      "                   all        553       4232      0.742      0.601      0.687      0.385\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/30      2.78G      1.517      1.281      1.136        140        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.2it/s 51.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.5it/s 1.7s.2s\n",
      "                   all        553       4232      0.754      0.632      0.707      0.403\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/30      2.78G      1.491       1.24      1.123         95        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.3it/s 50.7s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.1it/s 1.8s.2s\n",
      "                   all        553       4232      0.758      0.657      0.724      0.417\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/30      2.78G      1.478      1.221      1.119         91        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.0it/s 52.2s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.0it/s 1.8s.1s\n",
      "                   all        553       4232      0.772      0.647      0.728      0.421\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/30      2.78G      1.461      1.198      1.112        144        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.2it/s 50.8s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.7it/s 1.7s.2s\n",
      "                   all        553       4232      0.761      0.669      0.732      0.424\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/30      2.78G      1.449      1.182      1.104        156        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.2it/s 50.9s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.6it/s 1.7s.2s\n",
      "                   all        553       4232      0.772      0.667      0.738      0.428\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/30      2.78G      1.447      1.173      1.104        127        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.4it/s 49.9s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.5it/s 1.7s.2s\n",
      "                   all        553       4232      0.781      0.669      0.746      0.438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/30      2.78G      1.436      1.157      1.098        134        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.1it/s 51.4s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 9.9it/s 1.8s0.2s\n",
      "                   all        553       4232      0.789      0.665      0.749      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/30      2.78G      1.422      1.145      1.096        112        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.3it/s 50.5s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.1it/s 1.8s.2s\n",
      "                   all        553       4232      0.798      0.665      0.755      0.442\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/30      2.78G      1.415      1.132      1.092        114        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.2it/s 51.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.1it/s 1.8s.2s\n",
      "                   all        553       4232       0.78      0.679      0.756      0.443\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/30      2.78G      1.412      1.132       1.09         76        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.1it/s 51.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.1it/s 1.8s.1s\n",
      "                   all        553       4232      0.784       0.68      0.758      0.449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/30      2.78G      1.414      1.121       1.09        155        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.2it/s 50.9s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.2it/s 1.8s.2s\n",
      "                   all        553       4232      0.791      0.679      0.761      0.453\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/30      2.78G       1.41      1.117      1.089        157        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.0it/s 52.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.0it/s 1.8s.2s\n",
      "                   all        553       4232      0.795      0.676      0.762      0.453\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/30      2.78G      1.325      1.013      1.058         67        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.3it/s 50.5s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 9.8it/s 1.8s0.1s\n",
      "                   all        553       4232      0.782      0.686      0.757      0.451\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/30      2.78G      1.317     0.9891      1.056         79        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.7it/s 48.5s<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.1it/s 1.8s.2s\n",
      "                   all        553       4232      0.797      0.678      0.763      0.457\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/30      2.78G      1.311      0.981      1.051         93        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.5it/s 49.2s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.3it/s 1.8s.2s\n",
      "                   all        553       4232      0.797      0.684      0.764      0.457\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/30      2.78G      1.317     0.9773      1.055        100        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.5it/s 49.1s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.3it/s 1.8s.2s\n",
      "                   all        553       4232        0.8      0.678      0.763      0.456\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/30      2.78G      1.305     0.9713      1.053         69        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.6it/s 49.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 9.9it/s 1.8s0.2s\n",
      "                   all        553       4232      0.801      0.682      0.766      0.457\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/30      2.78G      1.306     0.9716      1.052         74        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.6it/s 49.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.5it/s 1.7s.2s\n",
      "                   all        553       4232        0.8      0.682      0.767      0.461\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/30      2.78G      1.309     0.9691      1.052         91        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.6it/s 48.8s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.1it/s 1.8s.2s\n",
      "                   all        553       4232      0.806       0.68      0.766      0.461\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/30      2.78G      1.296     0.9615      1.047         65        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.8it/s 47.6s<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 9.9it/s 1.8s0.2s\n",
      "                   all        553       4232      0.805      0.684      0.768       0.46\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/30      2.78G      1.297     0.9647       1.05         43        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.9it/s 47.6ss<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.2it/s 1.8s.1s\n",
      "                   all        553       4232      0.805      0.682      0.768      0.462\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/30      2.78G      1.297     0.9617      1.049         79        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 469/469 9.9it/s 47.5s<0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 10.4it/s 1.7s.1s\n",
      "                   all        553       4232      0.806      0.683      0.769      0.462\n",
      "\n",
      "30 epochs completed in 0.440 hours.\n",
      "Optimizer stripped from /home/igris/Files/Image processing projects/thermal_obj_detect/runs/detect/train2/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/igris/Files/Image processing projects/thermal_obj_detect/runs/detect/train2/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/igris/Files/Image processing projects/thermal_obj_detect/runs/detect/train2/weights/best.pt...\n",
      "Ultralytics 8.3.241 üöÄ Python-3.11.14 torch-2.9.1+cu130 CUDA:0 (NVIDIA GeForce RTX 3060, 11909MiB)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18/18 8.0it/s 2.3s0.2s\n",
      "                   all        553       4232      0.804      0.684      0.768      0.462\n",
      "                   car        479       2346       0.83      0.773      0.851      0.566\n",
      "                person        388       1886      0.778      0.595      0.686      0.358\n",
      "Speed: 0.2ms preprocess, 1.7ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/igris/Files/Image processing projects/thermal_obj_detect/runs/detect/train2\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "‚úì TRAINING COMPLETE!\n",
      "======================================================================\n",
      "Ready to train model (uncomment code above and set data.yaml path)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Train Model\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(model, data_yaml=Config.DATA_YAML, epochs=Config.EPOCHS):\n",
    "    \"\"\"\n",
    "    Train YOLOv8 model on thermal dataset\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STARTING MODEL TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    hyperparams = {\n",
    "        \"batch\": Config.BATCH_SIZE,\n",
    "        \"epochs\": epochs,\n",
    "        \"imgsz\": Config.IMG_SIZE,\n",
    "        \"optimizer\": Config.OPTIMIZER,\n",
    "        \"cos_lr\": True,\n",
    "        \"lr0\": Config.LEARNING_RATE,\n",
    "        \"warmup_epochs\": Config.WARMUP_EPOCHS,\n",
    "    }\n",
    "    \n",
    "    # Combine with augmentation parameters\n",
    "    training_params = {**hyperparams, **Config.AUGMENTATION}\n",
    "    \n",
    "    print(\"\\nTraining Configuration:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, value in training_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Start training\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"\\n‚úì Training on device: {device}\")\n",
    "    \n",
    "    results = model.train(\n",
    "        device=device,\n",
    "        data=data_yaml,\n",
    "        **training_params,\n",
    "        plots=True,  # Generate plots\n",
    "        save=True,   # Save checkpoints\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úì TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "# Uncomment and modify data_yaml path to train\n",
    "trained_model, results = train_model(model, data_yaml=\"data.yaml\", epochs=30)\n",
    "\n",
    "print(\"Ready to train model (uncomment code above and set data.yaml path)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dddb254-258c-465f-bb77-952185d51413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to evaluate model\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Evaluation Functions\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(model, data_yaml=Config.DATA_YAML):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = model.val(data=data_yaml)\n",
    "    \n",
    "    print(\"\\nüìä Performance Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  mAP@50:     {results.box.map50:.4f}\")\n",
    "    print(f\"  mAP@50-95:  {results.box.map:.4f}\")\n",
    "    print(f\"  Precision:  {results.box.p:.4f}\")\n",
    "    print(f\"  Recall:     {results.box.r:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to evaluate\n",
    "# eval_results = evaluate_model(model, data_yaml=\"path/to/data.yaml\")\n",
    "\n",
    "print(\"Ready to evaluate model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60591e0f-f17b-4ac6-8890-2f7153e8b123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for single image inference\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Single Image Inference\n",
    "# ============================================================================\n",
    "\n",
    "def inference_single_image(image_path, model, conf=Config.CONFIDENCE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Perform inference on a single image and display results\n",
    "    \"\"\"\n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    \n",
    "    # Read image\n",
    "    cv_image = cv2.imread(image_path)\n",
    "    if cv_image is None:\n",
    "        print(f\"‚úó Error: Could not read image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Perform detection\n",
    "    results = model(cv_image, conf=conf, verbose=False)\n",
    "    detections = Detections.from_ultralytics(results[0])\n",
    "    \n",
    "    print(f\"‚úì Detected {len(detections)} humans\")\n",
    "    \n",
    "    # Create annotators\n",
    "    box_annotator = sv.BoxAnnotator(thickness=2, color=sv.Color.GREEN)\n",
    "    label_annotator = sv.LabelAnnotator(text_thickness=2, text_scale=0.5)\n",
    "    \n",
    "    # Create labels\n",
    "    labels = [f\"Person {conf:.2f}\" for conf in detections.confidence]\n",
    "    \n",
    "    # Annotate\n",
    "    annotated = box_annotator.annotate(scene=cv_image.copy(), detections=detections)\n",
    "    annotated = label_annotator.annotate(scene=annotated, detections=detections, labels=labels)\n",
    "    \n",
    "    # Add count\n",
    "    cv2.putText(annotated, f\"Humans: {len(detections)}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Convert BGR to RGB for display\n",
    "    annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(annotated_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Detection Results - {len(detections)} Humans Detected')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save\n",
    "    output_path = f\"{Config.OUTPUT_DIR}/images/result_{Path(image_path).stem}.jpg\"\n",
    "    cv2.imwrite(output_path, annotated)\n",
    "    print(f\"‚úì Saved result to: {output_path}\")\n",
    "    \n",
    "    return detections, annotated\n",
    "\n",
    "# Example usage (replace with your image path):\n",
    "# detections, annotated = inference_single_image(\"path/to/your/image.jpg\", model)\n",
    "\n",
    "print(\"Ready for single image inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3adc0564-18d6-40a4-b7b2-82a73402dc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for batch image inference\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Batch Image Inference\n",
    "# ============================================================================\n",
    "\n",
    "def inference_batch_images(image_folder, model, conf=Config.CONFIDENCE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Process multiple images from a folder\n",
    "    \"\"\"\n",
    "    image_paths = list(Path(image_folder).glob(\"*.jpg\")) + \\\n",
    "                  list(Path(image_folder).glob(\"*.png\"))\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images in {image_folder}\")\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        print(f\"\\nProcessing {i+1}/{len(image_paths)}: {img_path.name}\")\n",
    "        detections, _ = inference_single_image(str(img_path), model, conf)\n",
    "        results_list.append({\n",
    "            'image': img_path.name,\n",
    "            'detections': len(detections)\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    df = pd.DataFrame(results_list)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"BATCH PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nTotal humans detected: {df['detections'].sum()}\")\n",
    "    print(f\"Average per image: {df['detections'].mean():.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# batch_results = inference_batch_images(\"path/to/image/folder\", model)\n",
    "\n",
    "print(\"Ready for batch image inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6ba8685-ad34-4b87-b3f3-1bec3aaa51f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 4710/4720 frames (99.8%)\n",
      "Current detections: 1\n",
      "\n",
      "‚úì Video processing complete!\n",
      "‚úì Output saved to: outputs/videos/output_test.mp4\n",
      "‚úì Average humans per frame: 4.09\n",
      "Ready for video processing\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Video Processing\n",
    "# ============================================================================\n",
    "\n",
    "def process_video(video_path, model, output_path=None, conf=Config.CONFIDENCE_THRESHOLD, \n",
    "                  display_every=30):\n",
    "    \"\"\"\n",
    "    Process video for human detection\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = f\"{Config.OUTPUT_DIR}/videos/output_{Path(video_path).stem}.mp4\"\n",
    "    \n",
    "    print(f\"üìπ Processing video: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚úó Error: Could not open video\")\n",
    "        return None\n",
    "    \n",
    "    # Video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video info: {width}x{height} @ {fps} FPS, {total_frames} frames\")\n",
    "    \n",
    "    # Output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    box_annotator = sv.BoxAnnotator(thickness=2, color=sv.Color.GREEN)\n",
    "    \n",
    "    frame_count = 0\n",
    "    detection_counts = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Detect\n",
    "        results = model(frame, conf=conf, verbose=False)\n",
    "        detections = Detections.from_ultralytics(results[0])\n",
    "        \n",
    "        # Annotate\n",
    "        annotated = box_annotator.annotate(scene=frame.copy(), detections=detections)\n",
    "        \n",
    "        # Add info\n",
    "        cv2.putText(annotated, f\"Humans: {len(detections)}\", (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(annotated, f\"Frame: {frame_count+1}/{total_frames}\", (10, 70),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        out.write(annotated)\n",
    "        detection_counts.append(len(detections))\n",
    "        frame_count += 1\n",
    "        \n",
    "        # Progress update\n",
    "        if frame_count % display_every == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Progress: {frame_count}/{total_frames} frames ({frame_count/total_frames*100:.1f}%)\")\n",
    "            print(f\"Current detections: {len(detections)}\")\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"\\n‚úì Video processing complete!\")\n",
    "    print(f\"‚úì Output saved to: {output_path}\")\n",
    "    print(f\"‚úì Average humans per frame: {np.mean(detection_counts):.2f}\")\n",
    "    \n",
    "    return detection_counts\n",
    "\n",
    "# Example usage:\n",
    "detection_counts = process_video(\"test.mp4\", model)\n",
    "\n",
    "print(\"Ready for video processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ae190a-7de6-4f82-9142-5838f9418195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to plot training history\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot Training History\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(csv_path='runs/detect/train/results.csv'):\n",
    "    \"\"\"Visualize training metrics\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Training History', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Box Loss\n",
    "        axes[0, 0].plot(df['train/box_loss'], label='Train', linewidth=2, color='blue')\n",
    "        axes[0, 0].set_title('Box Loss', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Class Loss\n",
    "        axes[0, 1].plot(df['train/cls_loss'], label='Train', linewidth=2, color='orange')\n",
    "        axes[0, 1].set_title('Classification Loss', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # mAP@50\n",
    "        axes[1, 0].plot(df['metrics/mAP50(B)'], label='mAP@50', linewidth=2, color='green')\n",
    "        axes[1, 0].set_title('mAP@50', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('mAP')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Precision & Recall\n",
    "        axes[1, 1].plot(df['metrics/precision(B)'], label='Precision', linewidth=2, color='blue')\n",
    "        axes[1, 1].plot(df['metrics/recall(B)'], label='Recall', linewidth=2, color='red')\n",
    "        axes[1, 1].set_title('Precision & Recall', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Score')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_path = f\"{Config.OUTPUT_DIR}/plots/training_history.png\"\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"‚úì Plot saved to: {output_path}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚úó Results file not found at {csv_path}\")\n",
    "        print(\"Train the model first!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")\n",
    "\n",
    "# Uncomment after training:\n",
    "# plot_training_history()\n",
    "\n",
    "print(\"Ready to plot training history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb245b6e-e102-4487-9539-46f041ab5f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to plot video detections\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot Video Detections\n",
    "# ============================================================================\n",
    "\n",
    "def plot_video_detections(detection_counts, output_path=None):\n",
    "    \"\"\"Plot detection counts over video frames\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = f\"{Config.OUTPUT_DIR}/plots/video_detections.png\"\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(detection_counts, linewidth=2, color='blue', alpha=0.7)\n",
    "    plt.fill_between(range(len(detection_counts)), detection_counts, alpha=0.3, color='blue')\n",
    "    \n",
    "    # Statistics\n",
    "    avg = np.mean(detection_counts)\n",
    "    max_det = np.max(detection_counts)\n",
    "    \n",
    "    plt.axhline(y=avg, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Average: {avg:.2f}')\n",
    "    plt.axhline(y=max_det, color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Maximum: {max_det}')\n",
    "    \n",
    "    plt.title('Human Detections Over Video Frames', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Frame Number', fontsize=12)\n",
    "    plt.ylabel('Number of Humans Detected', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úì Plot saved to: {output_path}\")\n",
    "\n",
    "# Use after processing video:\n",
    "# plot_video_detections(detection_counts)\n",
    "\n",
    "print(\"Ready to plot video detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de2ba596-fa34-4ea0-a6f8-2f7ee34b29af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for model comparison\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Model Comparison\n",
    "# ============================================================================\n",
    "\n",
    "def compare_models(model_names=['yolov8n.pt', 'yolov8s.pt'], data_yaml=Config.DATA_YAML):\n",
    "    \"\"\"Compare different YOLO model variants\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nüìä Evaluating {model_name}...\")\n",
    "        temp_model = YOLO(model_name)\n",
    "        val_results = temp_model.val(data=data_yaml, verbose=False)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'mAP@50': val_results.box.map50,\n",
    "            'mAP@50-95': val_results.box.map,\n",
    "            'Precision': val_results.box.p,\n",
    "            'Recall': val_results.box.r,\n",
    "            'Inference (ms)': val_results.speed['inference']\n",
    "        }\n",
    "    \n",
    "    df = pd.DataFrame(results).T\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPARISON RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df.to_string())\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # mAP comparison\n",
    "    df[['mAP@50', 'mAP@50-95']].plot(kind='bar', ax=axes[0], color=['green', 'blue'])\n",
    "    axes[0].set_title('mAP Comparison', fontweight='bold')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_xlabel('Model')\n",
    "    axes[0].legend(['mAP@50', 'mAP@50-95'])\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Speed comparison\n",
    "    df['Inference (ms)'].plot(kind='bar', ax=axes[1], color='orange')\n",
    "    axes[1].set_title('Inference Speed', fontweight='bold')\n",
    "    axes[1].set_ylabel('Time (ms)')\n",
    "    axes[1].set_xlabel('Model')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.OUTPUT_DIR}/plots/model_comparison.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save CSV\n",
    "    df.to_csv(f\"{Config.OUTPUT_DIR}/model_comparison.csv\")\n",
    "    print(f\"\\n‚úì Saved to: {Config.OUTPUT_DIR}/model_comparison.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Uncomment to compare:\n",
    "# comparison_df = compare_models(data_yaml=\"path/to/data.yaml\")\n",
    "\n",
    "print(\"Ready for model comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5482a9c4-d588-4214-a699-23ac99719516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for benchmarking\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: Benchmark Model Speed\n",
    "# ============================================================================\n",
    "\n",
    "def benchmark_model(model, image_path, num_runs=100):\n",
    "    \"\"\"Benchmark inference speed\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODEL BENCHMARKING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    cv_image = cv2.imread(image_path)\n",
    "    if cv_image is None:\n",
    "        print(f\"‚úó Error reading image: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Running {num_runs} iterations...\")\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        model(cv_image, conf=Config.CONFIDENCE_THRESHOLD, verbose=False)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for i in range(num_runs):\n",
    "        start = time.time()\n",
    "        model(cv_image, conf=Config.CONFIDENCE_THRESHOLD, verbose=False)\n",
    "        times.append(time.time() - start)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Progress: {i+1}/{num_runs}\")\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Statistics\n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    min_time = np.min(times)\n",
    "    max_time = np.max(times)\n",
    "    fps = 1 / avg_time\n",
    "    \n",
    "    print(\"\\nüìä Benchmark Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  Average time:  {avg_time*1000:.2f} ms\")\n",
    "    print(f\"  Std deviation: {std_time*1000:.2f} ms\")\n",
    "    print(f\"  Min time:      {min_time*1000:.2f} ms\")\n",
    "    print(f\"  Max time:      {max_time*1000:.2f} ms\")\n",
    "    print(f\"  FPS:           {fps:.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(np.array(times) * 1000, bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(avg_time * 1000, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {avg_time*1000:.2f} ms')\n",
    "    plt.title('Inference Time Distribution', fontweight='bold')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.OUTPUT_DIR}/plots/benchmark.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return {'avg_ms': avg_time * 1000, 'fps': fps}\n",
    "\n",
    "# Example:\n",
    "# benchmark_results = benchmark_model(model, \"path/to/test/image.jpg\")\n",
    "\n",
    "print(\"Ready for benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "357781c1-6912-4700-ab77-ac03c7415098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to export model\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 15: Export Model\n",
    "# ============================================================================\n",
    "\n",
    "def export_model(model, formats=['onnx']):\n",
    "    \"\"\"Export model for deployment\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODEL EXPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for fmt in formats:\n",
    "        print(f\"\\nüì¶ Exporting to {fmt.upper()}...\")\n",
    "        try:\n",
    "            model.export(format=fmt)\n",
    "            print(f\"‚úì Successfully exported to {fmt}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Export failed: {e}\")\n",
    "    \n",
    "    print(\"\\n‚úì Export complete!\")\n",
    "\n",
    "# Uncomment to export:\n",
    "# export_model(model, formats=['onnx', 'torchscript'])\n",
    "\n",
    "print(\"Ready to export model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
